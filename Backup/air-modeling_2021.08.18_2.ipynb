{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5fe3393",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9323b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers[torch]==4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7cd4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertModel, DistilBertModel\n",
    "from tqdm import tqdm\n",
    "# import kobert_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dddc1c",
   "metadata": {},
   "source": [
    "# bring pre-trained kobert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0165a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_bertmodel = BertModel.from_pretrained(\"monologg/kobert\")\n",
    "# 참고\n",
    "# 아레 코드랑 결과 같음\n",
    "# model_skt = BertModel.from_pretrained(\"skt/kobert-base-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6ef7fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Administrator\\\\Desktop\\\\Work\\\\2. 인공지능리서치AIR\\\\1. source code'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d73904",
   "metadata": {},
   "source": [
    "# data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03859cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/2. data sets/1. input data\"\n",
    "file_name = \"merged_data_set_AIR.xlsx\"\n",
    "data_set = pd.read_excel(os.path.join(data_path, file_name))\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99ce600",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.columns = ['no',\n",
    "                    'uid',\n",
    "                   'publisher',\n",
    "                   'title',\n",
    "                   'summary',\n",
    "                   'content',\n",
    "                   'content_url',\n",
    "                    'update_at',\n",
    "                   'importance',\n",
    "                   'polarity']\n",
    "data_set = data_set.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c46aab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>no</th>\n",
       "      <th>uid</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "      <th>content_url</th>\n",
       "      <th>update_at</th>\n",
       "      <th>importance</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>354652739944452416</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>2016-02-26T10:25:00</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  no                 uid publisher  \\\n",
       "0      0   0  354652739944452416       뉴스1   \n",
       "\n",
       "                                    title  \\\n",
       "0  김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'   \n",
       "\n",
       "                                             summary  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                             content  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                         content_url            update_at  \\\n",
       "0  http://news.naver.com/main/read.nhn?mode=LSD&m...  2016-02-26T10:25:00   \n",
       "\n",
       "  importance polarity  \n",
       "0          +        +  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7c4953b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "169dc0e5",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e434f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert_transformers import get_tokenizer\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d11fb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"[CLS] 한국어 모델을 공유합니다. [SEP]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c74059",
   "metadata": {},
   "source": [
    "## For title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bd45bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-ebfe296a4add>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_set_for_title[\"token_len\"] = \"\"\n",
      "<ipython-input-9-ebfe296a4add>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_set_for_title[\"tokenization\"] = \"\"\n"
     ]
    }
   ],
   "source": [
    "data_set_for_title = data_set[['uid', 'title']]\n",
    "data_set_for_title[\"token_len\"] = \"\"\n",
    "data_set_for_title[\"tokenization\"] = \"\"\n",
    "# data_set_for_title.reset_index()\n",
    "data_set_for_title = data_set_for_title.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fb35dea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>token_len</th>\n",
       "      <th>tokenization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>354652739944452421</td>\n",
       "      <td>김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                 uid                                   title  \\\n",
       "0      0  354652739944452421  김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'   \n",
       "\n",
       "  token_len tokenization  \n",
       "0                         "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_for_title.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "076accc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>token_len</th>\n",
       "      <th>tokenization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47999</th>\n",
       "      <td>47999</td>\n",
       "      <td>659762898834428055</td>\n",
       "      <td>\"최신 갤럭시·아이폰, 4명 중 1명이 빌려쓴다\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                 uid                        title token_len  \\\n",
       "47999  47999  659762898834428055  \"최신 갤럭시·아이폰, 4명 중 1명이 빌려쓴다\"             \n",
       "\n",
       "      tokenization  \n",
       "47999               "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_for_title.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bdb66496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/48000 [00:00<?, ?it/s]<ipython-input-74-9291174cddaf>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_set_for_title[\"token_len\"][i] = len(tokenized)\n",
      "<ipython-input-74-9291174cddaf>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_set_for_title[\"tokenization\"][i] = \",\".join(tokenized)\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 48000/48000 [00:26<00:00, 1833.64it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(data_set_for_title.shape[0])):\n",
    "    tokenized = tokenizer.tokenize(str(data_set_for_title['title'][i]))\n",
    "    data_set_for_title[\"token_len\"][i] = len(tokenized)\n",
    "    data_set_for_title[\"tokenization\"][i] = \",\".join(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "16f684ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>token_len</th>\n",
       "      <th>tokenization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>354652739944452421</td>\n",
       "      <td>김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'</td>\n",
       "      <td>22</td>\n",
       "      <td>▁김진,석,▁CJ,헬,로,비,전,▁대표,▁',주,주,총회,▁70%,이상,▁찬성,으로...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>848994285528420888</td>\n",
       "      <td>[포토] LG유플러스 AR글라스 국내 최초 출시</td>\n",
       "      <td>13</td>\n",
       "      <td>▁[,포토,],▁LG,유플러스,▁,AR,글,라,스,▁국내,▁최초,▁출시</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                 uid                                   title  \\\n",
       "0      0  354652739944452421  김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'   \n",
       "1      1  848994285528420888              [포토] LG유플러스 AR글라스 국내 최초 출시   \n",
       "\n",
       "  token_len                                       tokenization  \n",
       "0        22  ▁김진,석,▁CJ,헬,로,비,전,▁대표,▁',주,주,총회,▁70%,이상,▁찬성,으로...  \n",
       "1        13            ▁[,포토,],▁LG,유플러스,▁,AR,글,라,스,▁국내,▁최초,▁출시  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_for_title.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "16f39a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>token_len</th>\n",
       "      <th>tokenization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47998</th>\n",
       "      <td>47998</td>\n",
       "      <td>885647782860427818</td>\n",
       "      <td>귀한 몸 된 갤럭신S20 울트라…유통망 불만 높아진다</td>\n",
       "      <td>20</td>\n",
       "      <td>▁귀,한,▁몸,▁된,▁,갤,럭,신,S,20,▁울,트,라,...,유통,망,▁불,만,▁...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47999</th>\n",
       "      <td>47999</td>\n",
       "      <td>659762898834428055</td>\n",
       "      <td>\"최신 갤럭시·아이폰, 4명 중 1명이 빌려쓴다\"</td>\n",
       "      <td>17</td>\n",
       "      <td>▁\",최,신,▁갤럭시,·,아이,폰,,,▁4,명,▁중,▁1,명이,▁빌려,쓴,다,\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                 uid                          title token_len  \\\n",
       "47998  47998  885647782860427818  귀한 몸 된 갤럭신S20 울트라…유통망 불만 높아진다        20   \n",
       "47999  47999  659762898834428055    \"최신 갤럭시·아이폰, 4명 중 1명이 빌려쓴다\"        17   \n",
       "\n",
       "                                            tokenization  \n",
       "47998  ▁귀,한,▁몸,▁된,▁,갤,럭,신,S,20,▁울,트,라,...,유통,망,▁불,만,▁...  \n",
       "47999       ▁\",최,신,▁갤럭시,·,아이,폰,,,▁4,명,▁중,▁1,명이,▁빌려,쓴,다,\"  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_for_title.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af2841db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Administrator\\\\Desktop\\\\Work\\\\2. 인공지능리서치AIR\\\\1. source code'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e29c1b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"C:\\\\Users\\\\Administrator\\\\Desktop\\\\Work\\\\2. 인공지능리서치AIR\\\\2. data sets\\\\2. processed data\"\n",
    "data_set_for_title.to_excel(save_path + \"/tokenization_for_title.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106f1a1",
   "metadata": {},
   "source": [
    "## For summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c9b97b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "      <th>content_url</th>\n",
       "      <th>update_at</th>\n",
       "      <th>importance</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>354652739944452421</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>2016-02-26T10:25:00</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                 uid publisher  \\\n",
       "0      0  354652739944452421       뉴스1   \n",
       "\n",
       "                                    title  \\\n",
       "0  김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'   \n",
       "\n",
       "                                             summary  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                             content  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                         content_url            update_at  \\\n",
       "0  http://news.naver.com/main/read.nhn?mode=LSD&m...  2016-02-26T10:25:00   \n",
       "\n",
       "  importance polarity  \n",
       "0          +        +  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cc1073dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-81-eda7f4fac42b>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_set_for_summary[\"token_len\"] = \"\"\n",
      "<ipython-input-81-eda7f4fac42b>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_set_for_summary[\"tokenization\"] = \"\"\n"
     ]
    }
   ],
   "source": [
    "data_set_for_summary = data_set[['uid', 'summary']]\n",
    "data_set_for_summary[\"token_len\"] = \"\"\n",
    "data_set_for_summary[\"tokenization\"] = \"\"\n",
    "# data_set_for_title.reset_index()\n",
    "data_set_for_summary = data_set_for_summary.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c9807123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>summary</th>\n",
       "      <th>token_len</th>\n",
       "      <th>tokenization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>354652739944452421</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                 uid  \\\n",
       "0      0  354652739944452421   \n",
       "\n",
       "                                             summary token_len tokenization  \n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...                         "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_for_summary.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e56c6594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>summary</th>\n",
       "      <th>token_len</th>\n",
       "      <th>tokenization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47999</th>\n",
       "      <td>47999</td>\n",
       "      <td>659762898834428055</td>\n",
       "      <td>SK텔레콤 홍보 모델들이 스마트폰 렌탈 서비스 'T렌탈'을 이용하고 있는 모습.\\n...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                 uid  \\\n",
       "47999  47999  659762898834428055   \n",
       "\n",
       "                                                 summary token_len  \\\n",
       "47999  SK텔레콤 홍보 모델들이 스마트폰 렌탈 서비스 'T렌탈'을 이용하고 있는 모습.\\n...             \n",
       "\n",
       "      tokenization  \n",
       "47999               "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_for_summary.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1051e0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/48000 [00:00<?, ?it/s]<ipython-input-91-d3807f5634da>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_set_for_summary[\"token_len\"][i] = len(tokenized)\n",
      "<ipython-input-91-d3807f5634da>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_set_for_summary[\"tokenization\"][i] = \",\".join(tokenized)\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 48000/48000 [00:31<00:00, 1525.82it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(data_set_for_summary.shape[0])):\n",
    "    tokenized = tokenizer.tokenize(str(data_set_for_summary['summary'][i]))\n",
    "    data_set_for_summary[\"token_len\"][i] = len(tokenized)\n",
    "    data_set_for_summary[\"tokenization\"][i] = \",\".join(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "db92a337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>summary</th>\n",
       "      <th>token_len</th>\n",
       "      <th>tokenization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>354652739944452421</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>90</td>\n",
       "      <td>▁임,세,영,▁,기자,▁=,▁김진,석,▁CJ,헬,로,비,전,▁대표,가,▁26,일,▁...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                 uid  \\\n",
       "0      0  354652739944452421   \n",
       "\n",
       "                                             summary token_len  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...        90   \n",
       "\n",
       "                                        tokenization  \n",
       "0  ▁임,세,영,▁,기자,▁=,▁김진,석,▁CJ,헬,로,비,전,▁대표,가,▁26,일,▁...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_for_summary.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4b2b80e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>summary</th>\n",
       "      <th>token_len</th>\n",
       "      <th>tokenization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47999</th>\n",
       "      <td>47999</td>\n",
       "      <td>659762898834428055</td>\n",
       "      <td>SK텔레콤 홍보 모델들이 스마트폰 렌탈 서비스 'T렌탈'을 이용하고 있는 모습.\\n...</td>\n",
       "      <td>62</td>\n",
       "      <td>▁SK,텔레콤,▁홍보,▁모델,들이,▁스마트폰,▁,렌,탈,▁서비스,▁',T,렌,탈,'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                 uid  \\\n",
       "47999  47999  659762898834428055   \n",
       "\n",
       "                                                 summary token_len  \\\n",
       "47999  SK텔레콤 홍보 모델들이 스마트폰 렌탈 서비스 'T렌탈'을 이용하고 있는 모습.\\n...        62   \n",
       "\n",
       "                                            tokenization  \n",
       "47999  ▁SK,텔레콤,▁홍보,▁모델,들이,▁스마트폰,▁,렌,탈,▁서비스,▁',T,렌,탈,'...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_for_summary.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d04e7e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"C:\\\\Users\\\\Administrator\\\\Desktop\\\\Work\\\\2. 인공지능리서치AIR\\\\2. data sets\\\\2. processed data\"\n",
    "data_set_for_summary.to_excel(save_path + \"/tokenization_for_summary.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7375e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd33d5ca",
   "metadata": {},
   "source": [
    "## For content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "257a392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_tokened = data_set\n",
    "data_set_tokened[\"content_len\"] = data_set_tokened['content'].str.len()\n",
    "data_set_tokened[\"content_token_len\"] = \"\"\n",
    "data_set_tokened[\"content_tokenized\"] = \"\"\n",
    "data_set_tokened = data_set_tokened.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "281cd922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>no</th>\n",
       "      <th>uid</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "      <th>content_url</th>\n",
       "      <th>update_at</th>\n",
       "      <th>importance</th>\n",
       "      <th>polarity</th>\n",
       "      <th>content_len</th>\n",
       "      <th>content_token_len</th>\n",
       "      <th>content_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>354652739944452416</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>2016-02-26T10:25:00</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>175.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index  no                 uid publisher  \\\n",
       "0        0      0   0  354652739944452416       뉴스1   \n",
       "\n",
       "                                    title  \\\n",
       "0  김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'   \n",
       "\n",
       "                                             summary  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                             content  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                         content_url            update_at  \\\n",
       "0  http://news.naver.com/main/read.nhn?mode=LSD&m...  2016-02-26T10:25:00   \n",
       "\n",
       "  importance polarity  content_len content_token_len content_tokenized  \n",
       "0          +        +        175.0                                      "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_tokened.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9ba01c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/48000 [00:00<?, ?it/s]<ipython-input-97-6259c18d9197>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_set_tokened[\"content_token_len\"][i] = len(tokenized)\n",
      "<ipython-input-97-6259c18d9197>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_set_tokened[\"content_tokenized\"][i] = \",\".join(tokenized)\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 48000/48000 [00:53<00:00, 902.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(data_set_tokened.shape[0])):\n",
    "    tokenized = tokenizer.tokenize(str(data_set_tokened['content'][i]))\n",
    "    data_set_tokened[\"content_token_len\"][i] = len(tokenized)\n",
    "    data_set_tokened[\"content_tokenized\"][i] = \",\".join(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "21111648",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_set_tokened' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-7a544de6d06d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_set_tokened\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data_set_tokened' is not defined"
     ]
    }
   ],
   "source": [
    "data_set_tokened.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1dc6dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save_path = \"C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/2. data sets/2. processed data\"\n",
    "data_set_tokened.to_excel(data_save_path + \"/data_set_tokened.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf4b67c",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6754befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BertModel, DistilBertModel\n",
    "from tqdm import tqdm\n",
    "# import kobert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1398d7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>no</th>\n",
       "      <th>uid</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "      <th>content_url</th>\n",
       "      <th>update_at</th>\n",
       "      <th>importance</th>\n",
       "      <th>polarity</th>\n",
       "      <th>content_len</th>\n",
       "      <th>content_token_len</th>\n",
       "      <th>content_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>354652739944452416</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>2016-02-26T10:25:00</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>175.0</td>\n",
       "      <td>98</td>\n",
       "      <td>▁임,세,영,▁,기자,▁=,▁김진,석,▁CJ,헬,로,비,전,▁대표,가,▁26,일,▁...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  level_0  index  no                 uid publisher  \\\n",
       "0           0        0      0   0  354652739944452416       뉴스1   \n",
       "\n",
       "                                    title  \\\n",
       "0  김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'   \n",
       "\n",
       "                                             summary  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                             content  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                         content_url            update_at  \\\n",
       "0  http://news.naver.com/main/read.nhn?mode=LSD&m...  2016-02-26T10:25:00   \n",
       "\n",
       "  importance polarity  content_len  content_token_len  \\\n",
       "0          +        +        175.0                 98   \n",
       "\n",
       "                                   content_tokenized  \n",
       "0  ▁임,세,영,▁,기자,▁=,▁김진,석,▁CJ,헬,로,비,전,▁대표,가,▁26,일,▁...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_save_path = \"C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/2. data sets/2. processed data\"\n",
    "data_set_tokened = pd.read_excel(data_save_path + \"/data_set_tokened.xlsx\")\n",
    "data_set_tokened['content'] = data_set_tokened['content'].astype(str)\n",
    "data_set_tokened.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f46d3d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set_tokened.publisher.value_counts().to_excel(data_save_path + '/publisher_count.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c1445ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대기업 최초로 주 35시간 근무제를 도입한 신세계가 계열사 별로 한 달에 하루씩 휴무제를 시행하고 있는데요.  직원들은 본인 의사와 관계없이 강제로 연차휴가를 써야 해서 논란이 되고 있습니다.   가 취재했습니다.    신세계 계열사 직원들은 원하는 날 연차휴가를 쓰기 어렵습니다.  계열사 별로 매달 하루 정해진 날에 한꺼번에 쉬어야 하기 때문입니다.  1년 연차   가운데  을 이렇게 회사가 정해준 날에 써야 합니다.   신세계 계열사 직원    놀러 가고 싶을 때가 있잖아요. 그럴 때  좀 사용을 하고 싶은데 아쉬운 면도 있었고  특히 급한 일이 있을 때 .    일부 계열사는 쉬는 날 연차를 쓰지 않는 직원들에게 다른 계열사 업무를 지원하게 합니다.   신세계 계열사 직원    주로 가면 마트 쪽에서 물건 포장하는  일하는 거 같아요. 연차 안 깎이려고 그렇게 나가서 일하는 거 같습니다.    신세계 측은 자원자에 한해서 계열사 업무지원이 이뤄지고   노사협의회 합의를 통해 일괄 휴가를 실시하고 있다고 말합니다.   최진용 신세계 SSG닷컴 인사팀장    특정일 날 다 같이 쉬는 게 사원들 입장에  서는 실제로 쉰다는 느낌을 가장 잘 받  을 수 있거든요. 주변에서 연락 오는 것도  없고요.    하지만 신세계 계열사 30여 곳 가운데 이마트 등 3곳을 제외하고는 노조가 없어   노사협의회 결정 자체도 논란의 소지가 있다는 지적입니다.    근로기준법에는 근로자가 원하는 날 휴가를 주도록 규정돼 있습니다.   '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_tokened.loc[9250, 'content_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc634176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‘삼성 인사…고동진 부사장, 삼성전자 무선사업부 사장 승진’\\n\\n삼성그룹은 1일 삼성전자 고동진 부사장을 삼성전자 무선사업부 사장으로 승진해 발표했다고 경향신문이 보도했다.\\n\\n고동진 사장은 1961년생으로 성균관대 산업공학과를 나와 영국 서섹스대에서 기술정책으로 석사학위를 받았다.\\n\\n1984년 삼성전자 개발관리과로 입사해 통신연구소 연구운영팀, 인사팀, 인력팀, 정보통신총괄 유럽연구소장, 무선사업부 해외상품기획그룹장, 무선사업부 개발관리팀장, 무선사업부 기술전략팀장을 거쳤다. 고동진 사장은 지난해 12월부터 무선사업부 개발실장을 맡고 있었다.\\n\\n삼성그룹은 고동진 사장 선임에 대해 “기술기획 업무를 시작으로 정보통신부문의 유럽연구소장을 역임한 후 무선사업부로 자리를 옮겨 상품기획, 기술전략 등 다양한 업무를 두루 경험하며 갤럭시의 성공신화를 이끌어 온 인물”이라며 “특히 2014년말 무선사업부 개발실장으로 부임해 갤럭시 S6, 노트5 등 차별화된 플래그십 모델 개발을 선도했다”고 밝혔다.\\n\\n삼성그룹은 또한 “고 사장은 H/W 및 S/W는 물론 KNOX, 삼성페이 등 솔루션 및 서비스 개발에도 폭넓은 안목과 식견을 갖추고 있어 무선사업의 제2 도약을 이끌 수 있을 것으로 기대된다”고 밝혔다.\\n\\n스포츠경향 뉴스를 네이버 뉴스 스탠드에서 만나 보세요. \\n모바일 경향 [ |  |  ] | 공식 SNS 계정'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_tokened.loc[5750, 'content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23c1099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_tokened['content_1'] = data_set_tokened['content']\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('연합뉴스TV.*jebo23', ' ', regex = True)\n",
    "#data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+=[가-힣]+ 기자', ' ', regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb5acef",
   "metadata": {},
   "source": [
    "## Backup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc6802",
   "metadata": {},
   "source": [
    "### Exact matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee44fd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_set_tokened' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6397d9e089a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#         cleaned = s.string[0:(s.start() - 1)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#         data_set_tokened.loc[row_idx, 'content_1'] = cleaned\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdata_set_tokened\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content_1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_set_tokened\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content_1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'이데일리 기자들의 비밀공간'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mdata_set_tokened\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content_1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_set_tokened\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content_1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'이데일리'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"[(]출처: [a-zA-Z0-9가-힣]+[)]\", ' ', regex = True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_set_tokened' is not defined"
     ]
    }
   ],
   "source": [
    "# data_set_tokened['content_1'] = data_set_tokened['content_1']\n",
    "\n",
    "## 신문사별\n",
    "### 1 이데일리\n",
    "# for row_idx, row in data_set_tokened.iterrows():\n",
    "#     raw = row['content_1']\n",
    "#     s = re.search(r'[가-힣]+ [(]+[a-zA-Z0-9.]+[@][a-zA-Z.]+[)]', raw)\n",
    "#     if s is None:\n",
    "#         data_set_tokened.loc[row_idx, 'content_1'] = raw\n",
    "#     else:\n",
    "#         cleaned = s.string[0:(s.start() - 1)]\n",
    "#         data_set_tokened.loc[row_idx, 'content_1'] = cleaned\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('이데일리 기자들의 비밀공간', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('이데일리', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"[(]출처: [a-zA-Z0-9가-힣]+[)]\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"단위 : 천원\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"화면번호 : [0-9]+\", ' ', regex = True)\n",
    "\n",
    "\n",
    "### 2 머니투데이\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"이미지/사진제공=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진제공=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "\n",
    "\n",
    "### 3 연합뉴스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('대담=', ' ', regex = False) #2454\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('정리=', ' ', regex = False) #2454\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('사진=', ' ', regex = False) #2454\n",
    "\n",
    "\n",
    "### 4 한국경제\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('한경로보뉴스', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[표]', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[그래프]', ' ', regex = False) # 213 \n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ 한경닷컴 기자', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('한경닷컴 [가-힣]+ 기자', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(현지시간)', ' ', regex = False) # 213 \n",
    "\n",
    "### 5 아시아경제\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('아시아경제 [가-힣]+ 기자', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('asiae.co.kr', \" \", regex=False)\n",
    "\n",
    "### 6 뉴시스\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"【[가-힣]+=뉴시스】\", ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"공감언론 뉴시스통신사\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(현지시각)', ' ', regex = False) # 213 \n",
    "\n",
    "\n",
    "### 7 서울경제\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[서울경제]', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('서울경제', ' ', regex = False) # 213 \n",
    "\n",
    "\n",
    "### 8 한국경제TV\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('ⓒ 한국경제TV, 무단 전재 및 재배포 금지', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('한국경제TV', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('무단 전재 및 재배포 금지', ' ', regex = False) # 213 \n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('그림 [0-9]+', ' ', regex = True) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('무단 전재 및 재배포 금지', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('라이온봇기자', ' ', regex = False) #213\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('※ 본 기사는 한국경제TV와 `금융 AI 전문기업 씽크풀`이 실시간으로 작성한 기사입니다.', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('동영상 뉴스', ' ', regex = False) #213\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('<기자>', ' ', regex = False) #213\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[인터뷰]', ' ', regex = False) #213\n",
    "\n",
    "### 9 파이낸셜뉴스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('\\'fnRASSI\\'는 금융 AI 전문기업 씽크풀과 파이낸셜뉴스의 협업으로 로봇기자가 실시간으로 생산하는 기사입니다.', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('fnRASSI@fnnews.com fnRASSI', ' ', regex = False) # 213 \n",
    "\n",
    "### 10 매일경제\n",
    "### 11 헤럴드경제\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"헤럴드경제=[가-힣]+ 기자\", ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"헤럴드경제 [가-힣]+ 기자\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"헤럴드경제\", ' ', regex = False)\n",
    "\n",
    "### 12 뉴스1\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z가-힣()]+=뉴스1', ' ', regex = True) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('뉴스1', ' ', regex = False) # 213 \n",
    "\n",
    "### 13 전자신문\n",
    "# for row_idx, row in data_set_tokened.iterrows():\n",
    "#     raw = row['content_1']\n",
    "#     s = re.search(r'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', raw)\n",
    "#     if s is None:\n",
    "#         data_set_tokened.loc[row_idx, 'content_1'] = raw\n",
    "#     else:\n",
    "#         cleaned = s.string[0:(s.start() - 1)]\n",
    "#         data_set_tokened.loc[row_idx, 'content_1'] = cleaned\n",
    "\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"전자신문과 씽크풀의 증시분석 전문기자 로봇 ET가 쓴 기사입니다\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"전자신문 관련뉴스해당 언론사에서 선정하며 로 이동해 볼 수 있습니다\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"개별 기사의 섹션 정보는 해당 언론사의 분류를 따르고 있습니다\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"기사의 섹션 정보는 해당 언론사의 분류를 따르고 있습니다\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"언론사는 개별 기사를 2개 이상 섹션으로 중복 분류할 수 있습니다\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"전자신문 관련뉴스언론사 페이지로 이동합니다\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"◆ Report statistics\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"◆ Report briefing\", ' ', regex = False)\n",
    "                \n",
    "        \n",
    "### 14 조선비즈\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('chosunbiz.com', ' ', regex = False)\n",
    "                \n",
    "### 15 국민일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('※ 이 기사는 국민일보와 엠로보가 개발한 증권뉴스 전용 인공지능 로봇 ‘스톡봇’이 금융감독원 전자공시시스템(DART)과 한국거래소(KRX) 데이터를 토대로 작성한 것입니다. 지속적인 업그레이드를 통해 더욱 풍부하고 정확한 내용을 담아 가겠습니다.', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('스톡봇 기자', ' ', regex = False) #213\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('공시 전문으로 이동', ' ', regex = False) #213\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+팀 [가-힣]+ [a-zA-Z0-9.]+[@][a-zA-Z.]+', ' ', regex = True)\n",
    "\n",
    "### 16 데일리안\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('데일리안 스팟뉴스2', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('데일리안 스팟뉴스팀', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('데일리안', ' ', regex = False)\n",
    "\n",
    "### 17 디지털타임스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('인터넷마케팅팀', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('인터넷 마케팅팀', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('디지털타임스', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('디지털뉴스부', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('문의:', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"[a-zA-Z0-9가-힣]+ 제공\", ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"핫 섹션\", ' ', regex = True)\n",
    "\n",
    "### 18 머니S\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('@머니S MNB', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"그래프=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진제공\", ' ', regex = False)\n",
    "\n",
    "### 19 동아일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"원본|\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"기획·제작|\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"소비자경제부\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ 인턴', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z0-9가-힣]+ 제공', \" \", regex=True)\n",
    "# for row_idx, row in data_set_tokened.iterrows():\n",
    "#     raw = row['content_1']\n",
    "#     s = re.search(r'특별취재팀', raw)\n",
    "#     if s is None:\n",
    "#         data_set_tokened.loc[row_idx, 'content_1'] = raw\n",
    "#     else:\n",
    "#         cleaned = s.string[0:(s.start() - 1)]\n",
    "#         data_set_tokened.loc[row_idx, 'content_1'] = cleaned\n",
    "\n",
    "### 20 중앙일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"중앙일보디자인\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"온라인 중앙일보\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"로컬편집기사 기자\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('글, 사진=', \" \", regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"[ⓒ 조인스랜드 : DramaHouse & J Content Hub Co.,Ltd.\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z가-힣]+=[a-zA-Z가-힣]+ 특파원', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('문의 [0-9]+=[0-9]+', \" \", regex=True)\n",
    "\n",
    "### 21 아이뉴스24\n",
    "\n",
    "### 22 부산일보\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진=[a-zA-Z0-9가-힣]+ 제공\", ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진-[a-zA-Z0-9가-힣]+ 제공\", ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[→]', \" 다음 \", regex=True)\n",
    "\n",
    "### 23 SBS CNBC\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"자세한 내용은 동영상을 시청하시기 바랍니다.\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"동영상 뉴스\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"경제와이드 이슈&\", ' ', regex = False)\n",
    "\n",
    "### 24 세계일보\n",
    "\n",
    "\n",
    "### 25 YTN\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('저작권자(c) YTN & YTN PLUS.', \" \", regex=False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ [\\[]+[a-zA-Z0-9.]+[@][a-zA-Z0-9.]+[\\]]', \" \", regex=True)\n",
    "\n",
    "### 26 한국일보\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('한국일보 [가-힣]+팀', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('한국스포츠경제', \" \", regex=False)\n",
    "       \n",
    "### 27 서울신문\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('온라인뉴스부', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('나우뉴스부', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('나우뉴스', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('재미있는 세상', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('서울비즈', \" \", regex=False)\n",
    "\n",
    "### 28 노컷뉴스\n",
    "### 29 조선일보\n",
    "### 30 경향신문\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('경향비즈 SNS', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('모바일 경향', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('모바일경향', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('공식 SNS 계정', \" \", regex=False)\n",
    "\n",
    "### 31 문화일보\n",
    "### 32 스포츠조선\n",
    "### 33 MBN뉴스\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집 : [가-힣]+', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집: [가-힣]+', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집:[가-힣]+', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집 : [가-힣]+', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재: [가-힣]+', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재:[가-힣]+', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재 : [가-힣]+', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('MBN뉴스 [가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('동영상 뉴스', \" \", regex=False)\n",
    "\n",
    "\n",
    "### 34 조세일보\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+[(][a-zA-Z0-9.-]+[@][a-zA-Z.]+[)]', ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ 전문위원', \" \", regex = True)\n",
    "\n",
    "### 35 머니위크\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+=[가-힣]+ 기자', ' ', regex = True) # 광주=이재호 기자\n",
    "\n",
    "### 36 한겨레\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('공식 SNS', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('Weconomy 홈페이지 바로가기:', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('http://www.hani.co.kr/arti/economy', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('Weconomy 페이스북 바로가기:', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('https://www.facebook.com/econohani', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('지금 여기', \" \", regex=False)\n",
    "\n",
    "### 37 스포츠서울\n",
    "### 38 연합뉴스TV\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('동영상 뉴스', ' ', regex = False)\n",
    "\n",
    "### 39 스포츠경향\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('스포츠경향 뉴스를 네이버 뉴스 스탠드에서 만나 보세요', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('모바일 경향', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('공식 SNS 계정', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('온라인뉴스팀', ' ', regex = False)\n",
    "\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[↑]', \" 상승 \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[→]', \" 다음 \", regex=True)\n",
    "\n",
    "\n",
    "# 공통\n",
    "## 기자\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+ 선임기자', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+ 유통전문기자', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+ 객원기자', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+ 인턴기자', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('/[가-힣]+기자', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+기자', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+ 기자', \" \", regex=True)\n",
    "\n",
    "### 사진 출처\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('자료사진=[a-zA-Z0-9가-힣]+', ' ', regex = True) # 광주=이재호 기자\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진제공=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"제공=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"자료=[a-zA-Z0-9가-힣]+\", ' ', regex = True) # 213 \n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진=[a-zA-Z0-9가-힣 ]+ 제공\", ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "\n",
    "## e-mail address\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ [(]+[a-zA-Z0-9.]+[@][a-zA-Z0-9.]+[)]', ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ [a-zA-Z0-9.]+[@][a-zA-Z0-9.]+', ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+[(]+[a-zA-Z0-9.]+[@][a-zA-Z0-9.]+[)]', ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z0-9.]+[@][a-zA-Z0-9.]+', ' ', regex = True)\n",
    "\n",
    "## web site address\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z0-9]+[.][a-zA-Z0-9.]+[.][a-zA-Z0-9.]+', ' ', regex = True)\n",
    "\n",
    "## stock fs ticker no\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[(][0-9]{6}[)]', ' ', regex = True)\n",
    "# yyyy.mm.dd\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[0-9]+[.][0-9]+[.][0-9]+', ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[0-9]+일', \" \", regex=True)\n",
    "# 전화번호 (00-000-0000)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[0-9]+[-][0-9]+[-][0-9]+', ' ', regex = True)\n",
    "\n",
    "\n",
    "## 사진 설명\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(가운데)', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(오른쪽)', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(사진 오른쪽)', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(가장 오른쪽)', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(왼쪽)', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(왼쪽 [가-힣]+번째)', ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(가장 왼쪽)', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(왼쪽부터)', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[표]', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[그래프]', ' ', regex = False) # 213 \n",
    "\n",
    "## 기타 특수문자 처리\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[()·]', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[【】\\[\\]▽△▲▲■◇♦◆○●ⓒΔ▷▶�━ㆍ㈜]', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('\\n', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('\\u3000', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(u'\\xa0', u\" \", regex=True)\n",
    "\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[-=+,#/\\/\\?:^$@*\\\"※~&%ㆍ!』\\\\‘|\\[\\]\\<\\>`\\'…’》│]', \" \", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b153731",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ecc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 신문사별\n",
    "### 1 이데일리\n",
    "for row_idx, row in data_set_tokened.iterrows():\n",
    "    raw = row['content_1']\n",
    "    s = re.search(r'[가-힣]+ [(]+[a-zA-Z0-9.]+[@][a-zA-Z.]+[)]', raw)\n",
    "    if s is None:\n",
    "        data_set_tokened.loc[row_idx, 'content_1'] = raw\n",
    "    else:\n",
    "        cleaned = s.string[0:(s.start() - 1)]\n",
    "        data_set_tokened.loc[row_idx, 'content_1'] = cleaned\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('이데일리 기자들의 비밀공간', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('이데일리', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"[(]출처: [a-zA-Z0-9가-힣]+[)]\", ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"단위 : 천원\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"화면번호 : [0-9]+\", ' ', regex = True)\n",
    "\n",
    "\n",
    "### 2 머니투데이\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"이미지/사진제공=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진제공=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "\n",
    "\n",
    "### 3 연합뉴스\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('대담=', ' ', regex = False) #2454\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('정리=', ' ', regex = False) #2454\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('사진=', ' ', regex = False) #2454\n",
    "\n",
    "\n",
    "### 4 한국경제\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('한경로보뉴스', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[표]', ' ', regex = False) # 213 \n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[그래프]', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ 한경닷컴 기자', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('한경닷컴 [가-힣]+ 기자', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(현지시간)', ' ', regex = False) # 213 \n",
    "\n",
    "### 5 아시아경제\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('아시아경제 [가-힣]+ 기자', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('asiae.co.kr', \" \", regex=False)\n",
    "\n",
    "### 6 뉴시스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"【[가-힣]+=뉴시스】\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"공감언론 뉴시스통신사\", ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(현지시각)', ' ', regex = False) # 213 \n",
    "\n",
    "\n",
    "### 7 서울경제\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[서울경제]', ' ', regex = False) # 213 \n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('서울경제', ' ', regex = False) # 213 \n",
    "\n",
    "\n",
    "### 8 한국경제TV\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('ⓒ 한국경제TV, 무단 전재 및 재배포 금지', ' ', regex = False) # 213 \n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('한국경제TV', ' ', regex = False) # 213 \n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('무단 전재 및 재배포 금지', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('그림 [0-9]+', ' ', regex = True) # 213 \n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('무단 전재 및 재배포 금지', ' ', regex = False) # 213 \n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('라이온봇기자', ' ', regex = False) #213\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('※ 본 기사는 한국경제TV와 `금융 AI 전문기업 씽크풀`이 실시간으로 작성한 기사입니다.', ' ', regex = False) # 213 \n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('동영상 뉴스', ' ', regex = False) #213\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('<기자>', ' ', regex = False) #213\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[인터뷰]', ' ', regex = False) #213\n",
    "\n",
    "### 9 파이낸셜뉴스\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('\\'fnRASSI\\'는 금융 AI 전문기업 씽크풀과 파이낸셜뉴스의 협업으로 로봇기자가 실시간으로 생산하는 기사입니다.', ' ', regex = False) # 213 \n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('fnRASSI@fnnews.com fnRASSI', ' ', regex = False) # 213 \n",
    "\n",
    "### 10 매일경제\n",
    "### 11 헤럴드경제\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"헤럴드경제=[가-힣]+ 기자\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"헤럴드경제 [가-힣]+ 기자\", ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"헤럴드경제\", ' ', regex = False)\n",
    "\n",
    "### 12 뉴스1\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z가-힣()]+=뉴스1', ' ', regex = True) # 213 \n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('뉴스1', ' ', regex = False) # 213 \n",
    "\n",
    "### 13 전자신문\n",
    "for row_idx, row in data_set_tokened.iterrows():\n",
    "    raw = row['content_1']\n",
    "    s = re.search(r'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', raw)\n",
    "    if s is None:\n",
    "        data_set_tokened.loc[row_idx, 'content_1'] = raw\n",
    "    else:\n",
    "        cleaned = s.string[0:(s.start() - 1)]\n",
    "        data_set_tokened.loc[row_idx, 'content_1'] = cleaned\n",
    "\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"전자신문과 씽크풀의 증시분석 전문기자 로봇 ET가 쓴 기사입니다\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"전자신문 관련뉴스해당 언론사에서 선정하며 로 이동해 볼 수 있습니다\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"개별 기사의 섹션 정보는 해당 언론사의 분류를 따르고 있습니다\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"기사의 섹션 정보는 해당 언론사의 분류를 따르고 있습니다\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"언론사는 개별 기사를 2개 이상 섹션으로 중복 분류할 수 있습니다\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"전자신문 관련뉴스언론사 페이지로 이동합니다\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"◆ Report statistics\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"◆ Report briefing\", ' ', regex = False)\n",
    "                \n",
    "        \n",
    "### 14 조선비즈\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('chosunbiz.com', ' ', regex = False)\n",
    "                \n",
    "### 15 국민일보\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('※ 이 기사는 국민일보와 엠로보가 개발한 증권뉴스 전용 인공지능 로봇 ‘스톡봇’이 금융감독원 전자공시시스템(DART)과 한국거래소(KRX) 데이터를 토대로 작성한 것입니다. 지속적인 업그레이드를 통해 더욱 풍부하고 정확한 내용을 담아 가겠습니다.', ' ', regex = False) # 213 \n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('스톡봇 기자', ' ', regex = False) #213\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('공시 전문으로 이동', ' ', regex = False) #213\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+팀 [가-힣]+ [a-zA-Z0-9.]+[@][a-zA-Z.]+', ' ', regex = True)\n",
    "\n",
    "### 16 데일리안\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('데일리안 스팟뉴스2', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('데일리안 스팟뉴스팀', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('데일리안', ' ', regex = False)\n",
    "\n",
    "### 17 디지털타임스\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('인터넷마케팅팀', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('인터넷 마케팅팀', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('디지털타임스', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('디지털뉴스부', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('문의:', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"[a-zA-Z0-9가-힣]+ 제공\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"핫 섹션\", ' ', regex = True)\n",
    "\n",
    "### 18 머니S\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('@머니S MNB', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"그래프=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진제공\", ' ', regex = False)\n",
    "\n",
    "### 19 동아일보\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"원본|\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"기획·제작|\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"소비자경제부\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ 인턴', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z0-9가-힣]+ 제공', \" \", regex=True)\n",
    "for row_idx, row in data_set_tokened.iterrows():\n",
    "    raw = row['content_1']\n",
    "    s = re.search(r'특별취재팀', raw)\n",
    "    if s is None:\n",
    "        data_set_tokened.loc[row_idx, 'content_1'] = raw\n",
    "    else:\n",
    "        cleaned = s.string[0:(s.start() - 1)]\n",
    "        data_set_tokened.loc[row_idx, 'content_1'] = cleaned\n",
    "\n",
    "### 20 중앙일보\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"중앙일보디자인\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"온라인 중앙일보\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"로컬편집기사 기자\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('글, 사진=', \" \", regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"[ⓒ 조인스랜드 : DramaHouse & J Content Hub Co.,Ltd.\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z가-힣]+=[a-zA-Z가-힣]+ 특파원', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('문의 [0-9]+=[0-9]+', \" \", regex=True)\n",
    "\n",
    "### 21 아이뉴스24\n",
    "\n",
    "### 22 부산일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진=[a-zA-Z0-9가-힣]+ 제공\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진-[a-zA-Z0-9가-힣]+ 제공\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[→]', \" 다음 \", regex=True)\n",
    "\n",
    "### 23 SBS CNBC\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"자세한 내용은 동영상을 시청하시기 바랍니다.\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"동영상 뉴스\", ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"경제와이드 이슈&\", ' ', regex = False)\n",
    "\n",
    "### 24 세계일보\n",
    "\n",
    "\n",
    "### 25 YTN\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('저작권자(c) YTN & YTN PLUS.', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ [\\[]+[a-zA-Z0-9.]+[@][a-zA-Z0-9.]+[\\]]', \" \", regex=True)\n",
    "\n",
    "### 26 한국일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('한국일보 [가-힣]+팀', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('한국스포츠경제', \" \", regex=False)\n",
    "       \n",
    "### 27 서울신문\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('온라인뉴스부', \" \", regex=False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('나우뉴스부', \" \", regex=False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('나우뉴스', \" \", regex=False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('재미있는 세상', \" \", regex=False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('서울비즈', \" \", regex=False)\n",
    "\n",
    "### 28 노컷뉴스\n",
    "### 29 조선일보\n",
    "### 30 경향신문\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('경향비즈 SNS', \" \", regex=False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('모바일 경향', \" \", regex=False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('모바일경향', \" \", regex=False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('공식 SNS 계정', \" \", regex=False)\n",
    "\n",
    "### 31 문화일보\n",
    "### 32 스포츠조선\n",
    "### 33 MBN뉴스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집 : [가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집: [가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집:[가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집 : [가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재: [가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재:[가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재 : [가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('MBN뉴스 [가-힣]+', \" \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집', \" \", regex=False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재', \" \", regex=False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('동영상 뉴스', \" \", regex=False)\n",
    "\n",
    "\n",
    "### 34 조세일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+[(][a-zA-Z0-9.-]+[@][a-zA-Z.]+[)]', ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ 전문위원', \" \", regex = True)\n",
    "\n",
    "### 35 머니위크\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+=[가-힣]+ 기자', ' ', regex = True) # 광주=이재호 기자\n",
    "\n",
    "\n",
    "### 36 한겨레\n",
    "### 37 스포츠서울\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('제공 [\\|] [가-힣]+', \" \", regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('스포츠서울 [가-힣]+기자', \" \", regex = True)\n",
    "\n",
    "\n",
    "### 38 연합뉴스TV\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('연합뉴스TV.*jebo23', ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('연합뉴스TV.*yjebo@yna.co.kr', ' ', regex = True)\n",
    "\n",
    "### 38 KBS 뉴스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('KBS 뉴스.*입니다.', ' ', regex = True)\n",
    "\n",
    "### 39 스포츠경향\n",
    "###\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[↑]', \" 상승 \", regex=True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[→]', \" 다음 \", regex=True)\n",
    "\n",
    "\n",
    "# 공통\n",
    "## 기자\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+ 선임기자', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+ 유통전문기자', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+ 객원기자', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+ 인턴기자', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('/[가-힣]+기자', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+기자', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+ 기자', \" \", regex=True)\n",
    "\n",
    "### 사진 출처\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('자료사진=[a-zA-Z0-9가-힣]+', ' ', regex = True) # 광주=이재호 기자\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진제공=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"제공=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"자료=[a-zA-Z0-9가-힣]+\", ' ', regex = True) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진=[a-zA-Z0-9가-힣 ]+ 제공\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "\n",
    "## e-mail address\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ [(]+[a-zA-Z0-9.]+[@][a-zA-Z0-9.]+[)]', ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ [a-zA-Z0-9.]+[@][a-zA-Z0-9.]+', ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+[(]+[a-zA-Z0-9.]+[@][a-zA-Z0-9.]+[)]', ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z0-9.]+[@][a-zA-Z0-9.]+', ' ', regex = True)\n",
    "\n",
    "## web site address\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z0-9]+[.][a-zA-Z0-9.]+[.][a-zA-Z0-9.]+', ' ', regex = True)\n",
    "\n",
    "## stock fs ticker no\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[(][0-9]{6}[)]', ' ', regex = True)\n",
    "# yyyy.mm.dd\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[0-9]+[.][0-9]+[.][0-9]+', ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[0-9]+일', \" \", regex=True)\n",
    "# 전화번호 (00-000-0000)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[0-9]+[-][0-9]+[-][0-9]+', ' ', regex = True)\n",
    "\n",
    "\n",
    "## 사진 설명\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(가운데)', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(오른쪽)', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(사진 오른쪽)', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(가장 오른쪽)', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(왼쪽)', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(왼쪽 [가-힣]+번째)', ' ', regex = True)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(가장 왼쪽)', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(왼쪽부터)', ' ', regex = False)\n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[표]', ' ', regex = False) # 213 \n",
    "# data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[그래프]', ' ', regex = False) # 213 \n",
    "\n",
    "## 기타 특수문자 처리\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[()·]', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[【】\\[\\]▽△▲▲■◇♦◆○●◎ⓒΔ▷▶�━ㆍ㈜]', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('\\n', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('\\u3000', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(u'\\xa0', u\" \", regex=True)\n",
    "\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[-=+,#/\\/\\?:^$@*\\\"※~&%ㆍ!』\\\\‘|\\[\\]\\<\\>`\\'…’》│“”]', \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c20a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0911be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9eebac5e",
   "metadata": {},
   "source": [
    "## 신문사별 특징문구(long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e3ade64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_tokened['content_1'] = data_set_tokened['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1dd06f",
   "metadata": {},
   "source": [
    "### Exact matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e02d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 신문사별\n",
    "### 1 이데일리\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('이데일리 기자들의 비밀공간', ' ', regex = False)\n",
    "### 2 머니투데이\n",
    "### 3 연합뉴스\n",
    "### 4 한국경제\n",
    "### 5 아시아경제\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('asiae.co.kr', \" \", regex = False)\n",
    "\n",
    "### 6 뉴시스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"공감언론 뉴시스통신사\", ' ', regex = False)\n",
    "\n",
    "### 7 서울경제\n",
    "### 8 한국경제TV\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('ⓒ 한국경제TV, 무단 전재 및 재배포 금지', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('무단 전재 및 재배포 금지', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('※ 본 기사는 한국경제TV와 `금융 AI 전문기업 씽크풀`이 실시간으로 작성한 기사입니다.', ' ', regex = False) # 213 \n",
    "\n",
    "### 9 파이낸셜뉴스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('\\'fnRASSI\\'는 금융 AI 전문기업 씽크풀과 파이낸셜뉴스의 협업으로 로봇기자가 실시간으로 생산하는 기사입니다.', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('fnRASSI@fnnews.com fnRASSI', ' ', regex = False) # 213 \n",
    "\n",
    "### 10 매일경제\n",
    "### 11 헤럴드경제\n",
    "### 12 뉴스1\n",
    "### 13 전자신문\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"전자신문과 씽크풀의 증시분석 전문기자 로봇 ET가 쓴 기사입니다\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"전자신문 관련뉴스해당 언론사에서 선정하며 로 이동해 볼 수 있습니다\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"개별 기사의 섹션 정보는 해당 언론사의 분류를 따르고 있습니다\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"기사의 섹션 정보는 해당 언론사의 분류를 따르고 있습니다\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"언론사는 개별 기사를 2개 이상 섹션으로 중복 분류할 수 있습니다\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"전자신문 관련뉴스언론사 페이지로 이동합니다\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"◆ Report statistics\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"◆ Report briefing\", ' ', regex = False)\n",
    "                \n",
    "### 14 조선비즈\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('chosunbiz.com', ' ', regex = False)\n",
    "                \n",
    "### 15 국민일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('※ 이 기사는 국민일보와 엠로보가 개발한 증권뉴스 전용 인공지능 로봇 ‘스톡봇’이 금융감독원 전자공시시스템(DART)과 한국거래소(KRX) 데이터를 토대로 작성한 것입니다. 지속적인 업그레이드를 통해 더욱 풍부하고 정확한 내용을 담아 가겠습니다.', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('공시 전문으로 이동', ' ', regex = False) #213\n",
    "\n",
    "### 16 데일리안\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('데일리안 스팟뉴스2', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('데일리안 스팟뉴스팀', ' ', regex = False)\n",
    "\n",
    "### 17 디지털타임스\n",
    "### 18 머니S\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('@머니S MNB', ' ', regex = False)\n",
    "\n",
    "### 19 동아일보\n",
    "### 20 중앙일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"[ⓒ 조인스랜드 : DramaHouse & J Content Hub Co.,Ltd.\", ' ', regex = False)\n",
    "\n",
    "### 21 아이뉴스24\n",
    "### 22 부산일보\n",
    "### 23 SBS CNBC\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"자세한 내용은 동영상을 시청하시기 바랍니다.\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"경제와이드 이슈&\", ' ', regex = False)\n",
    "\n",
    "### 24 세계일보\n",
    "### 25 YTN\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('저작권자(c) YTN & YTN PLUS.', \" \", regex=False)\n",
    "\n",
    "### 26 한국일보\n",
    "### 27 서울신문\n",
    "### 28 노컷뉴스\n",
    "### 29 조선일보\n",
    "### 30 경향신문\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('경향비즈 SNS', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('공식 SNS 계정', \" \", regex=False)\n",
    "\n",
    "### 31 문화일보\n",
    "### 32 스포츠조선\n",
    "### 33 MBN뉴스\n",
    "### 34 조세일보\n",
    "### 35 머니위크\n",
    "### 36 한겨레\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('Weconomy 홈페이지 바로가기:', \" \", regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('http://www.hani.co.kr/arti/economy', \" \", regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('Weconomy 페이스북 바로가기:', \" \", regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('https://www.facebook.com/econohani', \" \", regex = False)\n",
    "\n",
    "### 37 스포츠서울\n",
    "### 38 연합뉴스TV\n",
    "### 39 스포츠경향\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('스포츠경향 뉴스를 네이버 뉴스 스탠드에서 만나 보세요', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('공식 SNS 계정', ' ', regex = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf4359a",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78078a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 신문사별\n",
    "### 1 이데일리\n",
    "data_set_tokened_tmp = data_set_tokened[data_set_tokened[\"publisher\"] == \"이데일리\"]\n",
    "for row_idx, row in data_set_tokened_tmp.iterrows():\n",
    "    raw = row['content_1']\n",
    "    s = re.search(r'[가-힣]+ [(]+[a-zA-Z0-9.]+[@][a-zA-Z.]+[)]', raw)\n",
    "    if s is None:\n",
    "        data_set_tokened.loc[row_idx, 'content_1'] = raw\n",
    "    else:\n",
    "        cleaned = s.string[0:(s.start() - 1)]\n",
    "        data_set_tokened.loc[row_idx, 'content_1'] = cleaned\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"[(]출처: [a-zA-Z0-9가-힣]+[)]\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"화면번호 : [0-9]+\", ' ', regex = True)\n",
    "\n",
    "### 2 머니투데이\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"이미지/사진제공=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진제공=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "\n",
    "### 3 연합뉴스\n",
    "### 4 한국경제\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ 한경닷컴 기자', \" \", regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('한경닷컴 [가-힣]+ 기자', \" \", regex = True)\n",
    "\n",
    "### 5 아시아경제\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('아시아경제 [가-힣]+ 기자', \" \", regex = True)\n",
    "\n",
    "### 6 뉴시스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"【[가-힣]+=뉴시스】\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"공감언론 뉴시스통신사\", ' ', regex = True)\n",
    "\n",
    "### 7 서울경제\n",
    "\n",
    "### 8 한국경제TV\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('그림 [0-9]+', ' ', regex = True)\n",
    "\n",
    "### 9 파이낸셜뉴스\n",
    "### 10 매일경제\n",
    "### 11 헤럴드경제\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"헤럴드경제=[가-힣]+ 기자\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"헤럴드경제 [가-힣]+ 기자\", ' ', regex = True)\n",
    "\n",
    "### 12 뉴스1\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z가-힣()]+=뉴스1', ' ', regex = True)\n",
    "\n",
    "### 13 전자신문\n",
    "data_set_tokened_tmp = data_set_tokened[data_set_tokened[\"publisher\"] == \"전자신문\"]\n",
    "for row_idx, row in data_set_tokened_tmp.iterrows():\n",
    "    raw = row['content_1']\n",
    "    s = re.search(r'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', raw)\n",
    "    if s is None:\n",
    "        data_set_tokened.loc[row_idx, 'content_1'] = raw\n",
    "    else:\n",
    "        cleaned = s.string[0:(s.start() - 1)]\n",
    "        data_set_tokened.loc[row_idx, 'content_1'] = cleaned\n",
    "        \n",
    "### 14 조선비즈\n",
    "### 15 국민일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+팀 [가-힣]+ [a-zA-Z0-9.]+[@][a-zA-Z.]+', ' ', regex = True)\n",
    "\n",
    "### 16 데일리안\n",
    "### 17 디지털타임스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"[a-zA-Z0-9가-힣]+ 제공\", ' ', regex = True)\n",
    "\n",
    "### 18 머니S\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"그래프=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "\n",
    "### 19 동아일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ 인턴', \" \", regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z0-9가-힣]+ 제공', \" \", regex = True)\n",
    "\n",
    "data_set_tokened_tmp = data_set_tokened[data_set_tokened[\"publisher\"] == \"동아일보\"]\n",
    "for row_idx, row in data_set_tokened.iterrows():\n",
    "    raw = row['content_1']\n",
    "    s = re.search(r'특별취재팀', raw)\n",
    "    if s is None:\n",
    "        data_set_tokened.loc[row_idx, 'content_1'] = raw\n",
    "    else:\n",
    "        cleaned = s.string[0:(s.start() - 1)]\n",
    "        data_set_tokened.loc[row_idx, 'content_1'] = cleaned\n",
    "\n",
    "### 20 중앙일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z가-힣]+=[a-zA-Z가-힣]+ 특파원', \" \", regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('문의 [0-9]+=[0-9]+', \" \", regex = True)\n",
    "\n",
    "### 21 아이뉴스24\n",
    "### 22 부산일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진=[a-zA-Z0-9가-힣]+ 제공\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진-[a-zA-Z0-9가-힣]+ 제공\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[→]', \" 다음 \", regex = True)\n",
    "\n",
    "### 23 SBS CNBC\n",
    "### 24 세계일보\n",
    "### 25 YTN\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ [\\[]+[a-zA-Z0-9.]+[@][a-zA-Z0-9.]+[\\]]', \" \", regex = True)\n",
    "\n",
    "### 26 한국일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('한국일보 [가-힣]+팀', \" \", regex = True)\n",
    "       \n",
    "### 27 서울신문\n",
    "### 28 노컷뉴스\n",
    "### 29 조선일보\n",
    "### 30 경향신문\n",
    "### 31 문화일보\n",
    "### 32 스포츠조선\n",
    "### 33 MBN뉴스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집 : [가-힣]+', \" \", regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집: [가-힣]+', \" \", regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집:[가-힣]+', \" \", regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집 : [가-힣]+', \" \", regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재: [가-힣]+', \" \", regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재:[가-힣]+', \" \", regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재 : [가-힣]+', \" \", regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('MBN뉴스 [가-힣]+', \" \", regex = True)\n",
    "\n",
    "### 34 조세일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+[(][a-zA-Z0-9.-]+[@][a-zA-Z.]+[)]', ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ 전문위원', \" \", regex = True)\n",
    "\n",
    "### 35 머니위크\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+=[가-힣]+ 기자', ' ', regex = True) # 광주=이재호 기자\n",
    "\n",
    "### 36 한겨레\n",
    "### 37 스포츠서울\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('제공 [\\|] [가-힣]+', \" \", regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('스포츠서울 [가-힣]+기자', \" \", regex = True)\n",
    "\n",
    "### 38 연합뉴스TV\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('연합뉴스TV.*jebo23', ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('연합뉴스TV.*yjebo@yna.co.kr', ' ', regex = True)\n",
    "\n",
    "### 38 KBS 뉴스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('KBS 뉴스.*입니다.', ' ', regex = True)\n",
    "\n",
    "### 39 스포츠경향"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f7217",
   "metadata": {},
   "source": [
    "## 신문사별 특징문구(short)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b580240",
   "metadata": {},
   "source": [
    "### Exact mathchimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a945b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 신문사별\n",
    "### 1 이데일리\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('이데일리', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"단위 : 천원\", ' ', regex = False)\n",
    "\n",
    "### 2 머니투데이\n",
    "### 3 연합뉴스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('대담=', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('정리=', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('사진=', ' ', regex = False) \n",
    "\n",
    "### 4 한국경제\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('한경로보뉴스', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[표]', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[그래프]', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(현지시간)', ' ', regex = False)\n",
    "\n",
    "### 5 아시아경제\n",
    "### 6 뉴시스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(현지시각)', ' ', regex = False)\n",
    "\n",
    "### 7 서울경제\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[서울경제]', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('서울경제', ' ', regex = False)\n",
    "\n",
    "\n",
    "### 8 한국경제TV\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('한국경제TV', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('라이온봇기자', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('동영상 뉴스', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('<기자>', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[인터뷰]', ' ', regex = False)\n",
    "\n",
    "### 9 파이낸셜뉴스\n",
    "### 10 매일경제\n",
    "### 11 헤럴드경제\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"헤럴드경제\", ' ', regex = False)\n",
    "\n",
    "### 12 뉴스1\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('뉴스1', ' ', regex = False)\n",
    "\n",
    "### 13 전자신문\n",
    "### 14 조선비즈\n",
    "### 15 국민일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('스톡봇 기자', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('공시 전문으로 이동', ' ', regex = False)\n",
    "### 16 데일리안\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('데일리안 스팟뉴스2', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('데일리안 스팟뉴스팀', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('데일리안', ' ', regex = False)\n",
    "\n",
    "### 17 디지털타임스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('인터넷마케팅팀', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('인터넷 마케팅팀', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('디지털타임스', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('디지털뉴스부', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('문의:', ' ', regex = False)\n",
    "\n",
    "### 18 머니S\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진제공\", ' ', regex = False)\n",
    "\n",
    "### 19 동아일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"원본|\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"기획·제작|\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"소비자경제부\", ' ', regex = False)\n",
    "\n",
    "### 20 중앙일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"중앙일보디자인\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"온라인 중앙일보\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"로컬편집기사 기자\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('글, 사진=', \" \", regex = False)\n",
    "\n",
    "### 21 아이뉴스24\n",
    "### 22 부산일보\n",
    "### 23 SBS CNBC\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"동영상 뉴스\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"경제와이드 이슈&\", ' ', regex = False)\n",
    "\n",
    "### 24 세계일보\n",
    "### 25 YTN\n",
    "### 26 한국일보\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('한국스포츠경제', \" \", regex = False)\n",
    "       \n",
    "### 27 서울신문\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('온라인뉴스부', \" \", regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('나우뉴스부', \" \", regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('나우뉴스', \" \", regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('재미있는 세상', \" \", regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('서울비즈', \" \", regex = False)\n",
    "\n",
    "### 28 노컷뉴스\n",
    "### 29 조선일보\n",
    "### 30 경향신문\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('경향비즈 SNS', \" \", regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('모바일 경향', \" \", regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('모바일경향', \" \", regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('공식 SNS 계정', \" \", regex = False)\n",
    "\n",
    "### 31 문화일보\n",
    "### 32 스포츠조선\n",
    "### 33 MBN뉴스\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집', \" \", regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재', \" \", regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('동영상 뉴스', \" \", regex = False)\n",
    "\n",
    "\n",
    "### 34 조세일보\n",
    "### 35 머니위크\n",
    "### 36 한겨레\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('공식 SNS', \" \", regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('지금 여기', \" \", regex = False)\n",
    "\n",
    "### 37 스포츠서울\n",
    "### 38 연합뉴스TV\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('동영상 뉴스', ' ', regex = False)\n",
    "\n",
    "### 39 스포츠경향\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('모바일 경향', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('공식 SNS 계정', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('온라인뉴스팀', ' ', regex = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c23516b",
   "metadata": {},
   "source": [
    "## 공통"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17ba2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"단위 : 천원\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"화면번호 : [0-9]+\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"이미지/사진제공=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진제공=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('대담=', ' ', regex = False) #2454\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('정리=', ' ', regex = False) #2454\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('사진=', ' ', regex = False) #2454\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[표]', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[그래프]', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(현지시간)', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(현지시각)', ' ', regex = False) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('동영상 뉴스', ' ', regex = False) #213\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('<기자>', ' ', regex = False) #213\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[인터뷰]', ' ', regex = False) #213\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('공시 전문으로 이동', ' ', regex = False) #213\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('문의:', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"[a-zA-Z0-9가-힣]+ 제공\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"그래프=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진제공\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"원본|\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"기획·제작|\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('글, 사진=', \" \", regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z가-힣]+=[a-zA-Z가-힣]+ 특파원', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('문의 [0-9]+=[0-9]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진=[a-zA-Z0-9가-힣]+ 제공\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진-[a-zA-Z0-9가-힣]+ 제공\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[→]', \" 다음 \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"동영상 뉴스\", ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ [\\[]+[a-zA-Z0-9.]+[@][a-zA-Z0-9.]+[\\]]', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집 : [가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집: [가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집:[가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집 : [가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재: [가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재:[가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재 : [가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('MBN뉴스 [가-힣]+', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상편집', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('영상취재', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('동영상 뉴스', \" \", regex=False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[↑]', \" 상승 \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[→]', \" 다음 \", regex=True)\n",
    "\n",
    "## 기자\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+ 선임기자', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+ 유통전문기자', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+ 객원기자', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+ 인턴기자', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('/[가-힣]+기자', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+기자', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣·]+ 기자', \" \", regex=True)\n",
    "\n",
    "### 사진 및 자료 출처\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('자료사진=[a-zA-Z0-9가-힣]+', ' ', regex = True) # 광주=이재호 기자\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진제공=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"제공=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"자료=[a-zA-Z0-9가-힣]+\", ' ', regex = True) # 213 \n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진=[a-zA-Z0-9가-힣 ]+ 제공\", ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(\"사진=[a-zA-Z0-9가-힣]+\", ' ', regex = True)\n",
    "\n",
    "## e-mail address\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ [(]+[a-zA-Z0-9.]+[@][a-zA-Z0-9.]+[)]', ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+ [a-zA-Z0-9.]+[@][a-zA-Z0-9.]+', ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[가-힣]+[(]+[a-zA-Z0-9.]+[@][a-zA-Z0-9.]+[)]', ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z0-9.]+[@][a-zA-Z0-9.]+', ' ', regex = True)\n",
    "\n",
    "## web site address\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[a-zA-Z0-9]+[.][a-zA-Z0-9.]+[.][a-zA-Z0-9.]+', ' ', regex = True)\n",
    "\n",
    "## stock fs ticker no\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[(][0-9]{6}[)]', '', regex = True)\n",
    "\n",
    "# yyyy.mm.dd\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[0-9]+[.][0-9]+[.][0-9]+', ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[0-9]+일', \" \", regex=True)\n",
    "# 전화번호 (00-000-0000)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[0-9]+[-][0-9]+[-][0-9]+', ' ', regex = True)\n",
    "\n",
    "## 사진 설명\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(가운데)', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(오른쪽)', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(사진 오른쪽)', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(가장 오른쪽)', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(왼쪽)', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(왼쪽 [가-힣]+번째)', ' ', regex = True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(가장 왼쪽)', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('(왼쪽부터)', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[표]', ' ', regex = False)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[그래프]', ' ', regex = False)\n",
    "\n",
    "# 기타 특수문자 처리\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[()·]', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[【】\\[\\]▽△▲▲■◇♦◆○●ⓒΔ▷▶�━ㆍ㈜]', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('\\n', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('\\u3000', \" \", regex=True)\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace(u'\\xa0', u\" \", regex=True)\n",
    "\n",
    "data_set_tokened['content_1'] = data_set_tokened['content_1'].str.replace('[-=+,#/\\/\\?:^$@*\\\"※~&%ㆍ!』\\\\‘|\\[\\]\\<\\>`\\'…’》│]', \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ae7e4b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ead83002",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data_set_tokened.loc[:, ['uid', 'publisher', 'title', 'summary', 'content', 'content_1', 'content_url', 'update_at',\n",
    "                                       'importance', 'polarity',\n",
    "                                       'content_len']]\n",
    "data_cleaned[\"content_1_len\"] = data_cleaned['content_1'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e67a8f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "      <th>content_1</th>\n",
       "      <th>content_url</th>\n",
       "      <th>update_at</th>\n",
       "      <th>importance</th>\n",
       "      <th>polarity</th>\n",
       "      <th>content_len</th>\n",
       "      <th>content_1_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>354652739944452416</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>김진석 CJ헬로비전 대표가   오전 서울 마포구 상암동 누리꿈스퀘어에서 열린...</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>2016-02-26T10:25:00</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>175.0</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  uid publisher                                   title  \\\n",
       "0  354652739944452416       뉴스1  김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'   \n",
       "\n",
       "                                             summary  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                             content  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                           content_1  \\\n",
       "0      김진석 CJ헬로비전 대표가   오전 서울 마포구 상암동 누리꿈스퀘어에서 열린...   \n",
       "\n",
       "                                         content_url            update_at  \\\n",
       "0  http://news.naver.com/main/read.nhn?mode=LSD&m...  2016-02-26T10:25:00   \n",
       "\n",
       "  importance polarity  content_len  content_1_len  \n",
       "0          +        +        175.0            158  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fca02c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data_set_tokened.loc[:, ['uid', 'publisher', 'title', 'summary', 'content', 'content_1', 'content_url', 'update_at',\n",
    "                                       'importance', 'polarity',\n",
    "                                       'content_len']]\n",
    "data_cleaned[\"content_1_len\"] = data_cleaned['content_1'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fdec9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save_path = \"C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/2. data sets/2. processed data\"\n",
    "data_cleaned.to_excel(data_save_path + \"/data_cleaned.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f034d8",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cecf50",
   "metadata": {},
   "source": [
    "## Model1 : No preprocessing + KoBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab6a87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertModel, DistilBertModel\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95176f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>no</th>\n",
       "      <th>uid</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "      <th>content_url</th>\n",
       "      <th>update_at</th>\n",
       "      <th>importance</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>354652739944452416</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>2016-02-26T10:25:00</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  no                 uid publisher  \\\n",
       "0      0   0  354652739944452416       뉴스1   \n",
       "\n",
       "                                    title  \\\n",
       "0  김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'   \n",
       "\n",
       "                                             summary  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                             content  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                         content_url            update_at  \\\n",
       "0  http://news.naver.com/main/read.nhn?mode=LSD&m...  2016-02-26T10:25:00   \n",
       "\n",
       "  importance polarity  \n",
       "0          +        +  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "data_path = \"C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/2. data sets/1. input data\"\n",
    "file_name = \"merged_data_set_AIR.xlsx\"\n",
    "data_set = pd.read_excel(os.path.join(data_path, file_name))\n",
    "data_set.columns = ['no',\n",
    "                    'uid',\n",
    "                   'publisher',\n",
    "                   'title',\n",
    "                   'summary',\n",
    "                   'content',\n",
    "                   'content_url',\n",
    "                    'update_at',\n",
    "                   'importance',\n",
    "                   'polarity']\n",
    "data_set = data_set.reset_index()\n",
    "data_set['content'] = data_set['content'].astype(str)\n",
    "data_set.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2dee627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47241, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set = data_set.dropna(subset = [\"importance\"])\n",
    "data_set = data_set.reset_index(drop = True)\n",
    "data_set['index'] = data_set.index\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69956967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>no</th>\n",
       "      <th>uid</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "      <th>content_url</th>\n",
       "      <th>update_at</th>\n",
       "      <th>importance</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>354652739944452416</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>2016-02-26T10:25:00</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  no                 uid publisher  \\\n",
       "0      0   0  354652739944452416       뉴스1   \n",
       "\n",
       "                                    title  \\\n",
       "0  김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'   \n",
       "\n",
       "                                             summary  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                             content  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                         content_url            update_at  \\\n",
       "0  http://news.naver.com/main/read.nhn?mode=LSD&m...  2016-02-26T10:25:00   \n",
       "\n",
       "  importance polarity  \n",
       "0          +        +  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0efa3157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>no</th>\n",
       "      <th>uid</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "      <th>content_url</th>\n",
       "      <th>update_at</th>\n",
       "      <th>importance</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47240</th>\n",
       "      <td>47240</td>\n",
       "      <td>34499</td>\n",
       "      <td>659762898834428032</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>\"최신 갤럭시·아이폰, 4명 중 1명이 빌려쓴다\"</td>\n",
       "      <td>SK텔레콤 홍보 모델들이 스마트폰 렌탈 서비스 'T렌탈'을 이용하고 있는 모습.\\n...</td>\n",
       "      <td>SK텔레콤 홍보 모델들이 스마트폰 렌탈 서비스 'T렌탈'을 이용하고 있는 모습. S...</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>2018-06-17T09:00:00</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index     no                 uid publisher  \\\n",
       "47240  47240  34499  659762898834428032      연합뉴스   \n",
       "\n",
       "                             title  \\\n",
       "47240  \"최신 갤럭시·아이폰, 4명 중 1명이 빌려쓴다\"   \n",
       "\n",
       "                                                 summary  \\\n",
       "47240  SK텔레콤 홍보 모델들이 스마트폰 렌탈 서비스 'T렌탈'을 이용하고 있는 모습.\\n...   \n",
       "\n",
       "                                                 content  \\\n",
       "47240  SK텔레콤 홍보 모델들이 스마트폰 렌탈 서비스 'T렌탈'을 이용하고 있는 모습. S...   \n",
       "\n",
       "                                             content_url            update_at  \\\n",
       "47240  http://news.naver.com/main/read.nhn?mode=LSD&m...  2018-06-17T09:00:00   \n",
       "\n",
       "      importance polarity  \n",
       "47240          -        0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4710bcbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+': 1, '0': 0, '-': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding the labels\n",
    "possible_labels = data_set.importance.unique()\n",
    "encoded_values = [1, 0, 2]\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = encoded_values[index]\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51a431bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set['label'] = data_set.importance.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7e1eef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 47238, 47239, 47240], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9fe79ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, ..., 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b77d01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_set.index.values,\n",
    "                                                 data_set.label.values,\n",
    "                                                 test_size = 0.2,\n",
    "                                                 random_state = 42,\n",
    "                                                 stratify = data_set.label.values)\n",
    "data_set['data_type'] = ['not_set'] * data_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9543839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>no</th>\n",
       "      <th>uid</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "      <th>content_url</th>\n",
       "      <th>update_at</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>importance</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">+</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>11725</td>\n",
       "      <td>11725</td>\n",
       "      <td>11725</td>\n",
       "      <td>11725</td>\n",
       "      <td>11725</td>\n",
       "      <td>11669</td>\n",
       "      <td>11725</td>\n",
       "      <td>11725</td>\n",
       "      <td>11725</td>\n",
       "      <td>11724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>2932</td>\n",
       "      <td>2932</td>\n",
       "      <td>2932</td>\n",
       "      <td>2932</td>\n",
       "      <td>2932</td>\n",
       "      <td>2922</td>\n",
       "      <td>2932</td>\n",
       "      <td>2932</td>\n",
       "      <td>2932</td>\n",
       "      <td>2932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">-</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>19088</td>\n",
       "      <td>19088</td>\n",
       "      <td>19088</td>\n",
       "      <td>19088</td>\n",
       "      <td>19088</td>\n",
       "      <td>18898</td>\n",
       "      <td>19088</td>\n",
       "      <td>19088</td>\n",
       "      <td>19088</td>\n",
       "      <td>19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>4772</td>\n",
       "      <td>4772</td>\n",
       "      <td>4772</td>\n",
       "      <td>4772</td>\n",
       "      <td>4772</td>\n",
       "      <td>4733</td>\n",
       "      <td>4772</td>\n",
       "      <td>4772</td>\n",
       "      <td>4772</td>\n",
       "      <td>4772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>6979</td>\n",
       "      <td>6979</td>\n",
       "      <td>6979</td>\n",
       "      <td>6979</td>\n",
       "      <td>6979</td>\n",
       "      <td>6958</td>\n",
       "      <td>6979</td>\n",
       "      <td>6979</td>\n",
       "      <td>6979</td>\n",
       "      <td>6974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1745</td>\n",
       "      <td>1745</td>\n",
       "      <td>1745</td>\n",
       "      <td>1745</td>\n",
       "      <td>1745</td>\n",
       "      <td>1743</td>\n",
       "      <td>1745</td>\n",
       "      <td>1745</td>\n",
       "      <td>1745</td>\n",
       "      <td>1745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            index     no    uid  publisher  title  summary  \\\n",
       "importance label data_type                                                   \n",
       "+          1     train      11725  11725  11725      11725  11725    11669   \n",
       "                 val         2932   2932   2932       2932   2932     2922   \n",
       "-          2     train      19088  19088  19088      19088  19088    18898   \n",
       "                 val         4772   4772   4772       4772   4772     4733   \n",
       "0          0     train       6979   6979   6979       6979   6979     6958   \n",
       "                 val         1745   1745   1745       1745   1745     1743   \n",
       "\n",
       "                            content  content_url  update_at  polarity  \n",
       "importance label data_type                                             \n",
       "+          1     train        11725        11725      11725     11724  \n",
       "                 val           2932         2932       2932      2932  \n",
       "-          2     train        19088        19088      19088     19082  \n",
       "                 val           4772         4772       4772      4772  \n",
       "0          0     train         6979         6979       6979      6974  \n",
       "                 val           1745         1745       1745      1745  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.loc[X_train, 'data_type'] = 'train'\n",
    "data_set.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "data_set.groupby(['importance', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83d75c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁한국', '어', '▁모델', '을', '▁공유', '합니다']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer\n",
    "from kobert_transformers import get_tokenizer\n",
    "tokenizer = get_tokenizer()\n",
    "tokenizer.tokenize('한국어 모델을 공유합니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bd4d1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>no</th>\n",
       "      <th>uid</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "      <th>content_url</th>\n",
       "      <th>update_at</th>\n",
       "      <th>importance</th>\n",
       "      <th>polarity</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>354652739944452416</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>2016-02-26T10:25:00</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  no                 uid publisher  \\\n",
       "0      0   0  354652739944452416       뉴스1   \n",
       "\n",
       "                                    title  \\\n",
       "0  김진석 CJ헬로비전 대표 '주주총회 70%이상 찬성으로 합병안 가결'   \n",
       "\n",
       "                                             summary  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                             content  \\\n",
       "0  임세영 기자 = 김진석 CJ헬로비전 대표가 26일 오전 서울 마포구 상암동 누리꿈스...   \n",
       "\n",
       "                                         content_url            update_at  \\\n",
       "0  http://news.naver.com/main/read.nhn?mode=LSD&m...  2016-02-26T10:25:00   \n",
       "\n",
       "  importance polarity  label data_type  \n",
       "0          +        +      1     train  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05def95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    data_set[data_set.data_type=='train'].content.values, \n",
    "    add_special_tokens = True, \n",
    "    return_attention_mask = True, \n",
    "    pad_to_max_length = True, \n",
    "    max_length = 512, \n",
    "    padding = True,\n",
    "    truncation = True,\n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    data_set[data_set.data_type=='val'].content.values, \n",
    "    add_special_tokens = True, \n",
    "    return_attention_mask = True, \n",
    "    pad_to_max_length = True, \n",
    "    padding = True,\n",
    "    max_length = 512, \n",
    "    truncation = True,\n",
    "    return_tensors = 'pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc56ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(data_set[data_set.data_type=='train'].label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(data_set[data_set.data_type=='val'].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d74890bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(dataset_train, \"C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/2. data sets/2. processed data/dataset_train.pth\")\n",
    "# torch.save(dataset_val, \"C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/2. data sets/2. processed data/dataset_val.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ba7b62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"monologg/kobert\",\n",
    "                                                     num_labels = 3,\n",
    "                                                     output_attentions = False,\n",
    "                                                     output_hidden_states = False)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a581adaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d779e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler = RandomSampler(dataset_train), \n",
    "                              batch_size = batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler = SequentialSampler(dataset_val), \n",
    "                                   batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a68c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(dataloader_train, \"C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/2. data sets/2. processed data/dataloader_train.pth\")\n",
    "# torch.save(dataloader_validation, \"C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/2. data sets/2. processed data/dataloader_validation.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0de4a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer & scheduler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "epochs = 5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0edce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78fdb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "03967663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "33bbd0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7a967c7320432e8944dfdeef2d7e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49051d31587b4b42848ae84ed251e3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/296 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:79] data. DefaultCPUAllocator: not enough memory: you tried to allocate 201326592 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-ee64287cbed8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                   \u001b[1;34m'labels'\u001b[0m\u001b[1;33m:\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                  }       \n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1494\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1496\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1497\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 959\u001b[1;33m         embedding_output = self.embeddings(\n\u001b[0m\u001b[0;32m    960\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2041\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2043\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:79] data. DefaultCPUAllocator: not enough memory: you tried to allocate 201326592 bytes."
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    model.train()\n",
    "    loss_train_total = 0\n",
    "    progress_bar = tqdm(dataloader_train, \n",
    "                        desc = 'Epoch {:1d}'.format(epoch), \n",
    "                        leave = False, \n",
    "                        disable = False)\n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "    torch.save(model.state_dict(), f'C:\\\\Users\\\\Administrator\\\\Desktop\\\\Work\\\\2. 인공지능리서치AIR\\\\1. source code\\\\save_models\\\\20210809\\\\finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c78c359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0be613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "87521df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function f1_score at 0x0000012A98C1B4C0>\n"
     ]
    }
   ],
   "source": [
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af925a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbb0419a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "#                                                       num_labels=len(label_dict),\n",
    "#                                                       output_attentions=False,\n",
    "#                                                       output_hidden_states=False)\n",
    "model = BertForSequenceClassification.from_pretrained(\"monologg/kobert\",\n",
    "                                                     num_labels = 3,\n",
    "                                                     output_attentions = False,\n",
    "                                                     output_hidden_states = False)\n",
    "# model\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9988804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/4. model result/kobert_20210810/models/finetuned_BERT_epoch_1.model'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(model_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f0350b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/4. model result/kobert_20210810/models/'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b1f54d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0\n",
      "Accuracy: 1325/1745\n",
      "\n",
      "Class: +\n",
      "Accuracy: 560/2932\n",
      "\n",
      "Class: -\n",
      "Accuracy: 526/4772\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "# data_path = \"C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/2. data sets/1. input data\"\n",
    "# file_name = \"merged_data_set_AIR.xlsx\"\n",
    "# data_set = pd.read_excel()\n",
    "\n",
    "model_path = \"C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/4. model result/kobert_20210810/models/\"\n",
    "file_name = \"finetuned_BERT_epoch_1.model\"\n",
    "loaded_model = torch.load(os.path.join(model_path, file_name), map_location=torch.device('cpu'))\n",
    "model.load_state_dict(loaded_model, strict = False)\n",
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "43f562d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.load('C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/4. model result/kobert_20210810/predicted/predicted_BERT_epoch_1.pth')\n",
    "labels = torch.load('C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/4. model result/kobert_20210810/predicted/true_vals_BERT_epoch_1.pth')\n",
    "\n",
    "label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "labels_flat = labels.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1ed6fbb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1]\n",
      "               +    0     -  Actual_total\n",
      "+           2267  111   554          2932\n",
      "0            663  205   877          1745\n",
      "-            508  150  4114          4772\n",
      "Pred_total  3438  466  5545          9449\n",
      "\n",
      "\n",
      "정확도 = 0.6970049740713303\n",
      "f1 score = 0.6414391267808465\n",
      "총민감도 = 0.6717491251503575\n",
      "총정밀도 = 0.6137462755158521\n",
      "긍정민감도 = 0.7731923601637107\n",
      "부정민감도 = 0.8621123218776194\n",
      "긍정정밀도 = 0.6593949970913322\n",
      "부정정밀도 = 0.7419296663660956\n",
      "긍정치명오분류율 = 0.18894952251023192\n",
      "부정치명오분류율 = 0.10645431684828165\n",
      "===================================================================================\n",
      "\n",
      "[Epoch 2]\n",
      "               +    0     -  Actual_total\n",
      "+           2347  133   452          2932\n",
      "0            686  255   804          1745\n",
      "-            555  173  4044          4772\n",
      "Pred_total  3588  561  5300          9449\n",
      "\n",
      "\n",
      "정확도 = 0.7033548523653297\n",
      "f1 score = 0.6509005179964893\n",
      "총민감도 = 0.6803480396289459\n",
      "총정밀도 = 0.6238963943721942\n",
      "긍정민감도 = 0.8004774897680764\n",
      "부정민감도 = 0.8474434199497066\n",
      "긍정정밀도 = 0.6541248606465998\n",
      "부정정밀도 = 0.7630188679245283\n",
      "긍정치명오분류율 = 0.15416098226466576\n",
      "부정치명오분류율 = 0.11630343671416597\n",
      "===================================================================================\n",
      "\n",
      "[Epoch 3]\n",
      "               +     0     -  Actual_total\n",
      "+           2236   277   419          2932\n",
      "0            575   433   737          1745\n",
      "-            490   300  3982          4772\n",
      "Pred_total  3301  1010  5138          9449\n",
      "\n",
      "\n",
      "정확도 = 0.7038840088898296\n",
      "f1 score = 0.6345221430557444\n",
      "총민감도 = 0.6421944101270798\n",
      "총정밀도 = 0.6270310321632968\n",
      "긍정민감도 = 0.762619372442019\n",
      "부정민감도 = 0.8344509639564124\n",
      "긍정정밀도 = 0.6773704937897607\n",
      "부정정밀도 = 0.7750097314130011\n",
      "긍정치명오분류율 = 0.14290586630286495\n",
      "부정치명오분류율 = 0.10268231349538977\n",
      "===================================================================================\n",
      "\n",
      "[Epoch 4]\n",
      "               +     0     -  Actual_total\n",
      "+           2251   260   421          2932\n",
      "0            597   431   717          1745\n",
      "-            498   328  3946          4772\n",
      "Pred_total  3346  1019  5084          9449\n",
      "\n",
      "\n",
      "정확도 = 0.7014498888771299\n",
      "f1 score = 0.6345874443520035\n",
      "총민감도 = 0.6455875451110026\n",
      "총정밀도 = 0.6239559226165949\n",
      "긍정민감도 = 0.7677353342428377\n",
      "부정민감도 = 0.8269069572506287\n",
      "긍정정밀도 = 0.6727435744172146\n",
      "부정정밀도 = 0.7761605035405192\n",
      "긍정치명오분류율 = 0.14358799454297408\n",
      "부정치명오분류율 = 0.10435875943000839\n",
      "===================================================================================\n",
      "\n",
      "[Epoch 5]\n",
      "               +     0     -  Actual_total\n",
      "+           2237   305   390          2932\n",
      "0            597   493   655          1745\n",
      "-            503   432  3837          4772\n",
      "Pred_total  3337  1230  4882          9449\n",
      "\n",
      "\n",
      "정확도 = 0.6949941792782305\n",
      "f1 score = 0.627591931474363\n",
      "총민감도 = 0.6363820539310218\n",
      "총정밀도 = 0.619041330359854\n",
      "긍정민감도 = 0.7629604365620737\n",
      "부정민감도 = 0.8040653813914501\n",
      "긍정정밀도 = 0.6703626011387473\n",
      "부정정밀도 = 0.7859483818107333\n",
      "긍정치명오분류율 = 0.1330150068212824\n",
      "부정치명오분류율 = 0.10540653813914501\n",
      "===================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "save_path = 'C:/Users/Administrator/Desktop/Work/2. 인공지능리서치AIR/4. model result/kobert_20210810/predicted/'\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    preds = torch.load(save_path + f'predicted_BERT_epoch_{epoch}.pth')\n",
    "    labels = torch.load(save_path + f'true_vals_BERT_epoch_{epoch}.pth')\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    cm = pd.DataFrame(confusion_matrix(labels_flat, preds_flat, labels = [1, 0, 2]))\n",
    "    cm.loc['Pred_Total']= cm.sum()\n",
    "    cm.loc[:,'Acutal_Total'] = cm.sum(axis=1)\n",
    "    \n",
    "    cm.columns = ['+', '0', '-', 'Actual_total']\n",
    "    cm.index = ['+', '0', '-', 'Pred_total']\n",
    "    \n",
    "    \n",
    "    pos_sensitivity = cm.iloc[0, 0] / cm.iloc[0, 3]\n",
    "    neu_sensitivity = cm.iloc[1, 0] / cm.iloc[1, 3]\n",
    "    neg_sensitivity = cm.iloc[2, 2] / cm.iloc[2, 3]\n",
    "    \n",
    "    \n",
    "    pos_precision = cm.iloc[0, 0] / cm.iloc[3, 0]\n",
    "    neu_precision = cm.iloc[1, 1] / cm.iloc[3, 1]\n",
    "    neg_precision = cm.iloc[2, 2] / cm.iloc[3, 2]\n",
    "    \n",
    "    pos_fatal_error = cm.iloc[0, 2] / cm.iloc[0, 3]\n",
    "    neg_fatal_error = cm.iloc[2, 0] / cm.iloc[2, 3]\n",
    "    \n",
    "    accuracy = (cm.iloc[0, 0] + cm.iloc[1, 1] + cm.iloc[2, 2]) / cm.iloc[3, 3]\n",
    "    sensitivity = (pos_sensitivity + neu_sensitivity + neg_sensitivity) / 3\n",
    "    precision = (pos_precision + neu_precision + neg_precision) / 3\n",
    "    \n",
    "    f1_score = 2 * ((precision * sensitivity) / (precision + sensitivity))\n",
    "    \n",
    "    \n",
    "    print(f\"[Epoch {epoch}]\")\n",
    "    print(cm)\n",
    "    print(\"\\n\")\n",
    "    print(f\"정확도 = {accuracy}\")\n",
    "    print(f\"f1 score = {f1_score}\")\n",
    "    print(f\"총민감도 = {sensitivity}\")\n",
    "    print(f\"총정밀도 = {precision}\")\n",
    "    \n",
    "    print(f\"긍정민감도 = {pos_sensitivity}\")\n",
    "    print(f\"부정민감도 = {neg_sensitivity}\")\n",
    "    \n",
    "    print(f\"긍정정밀도 = {pos_precision}\")\n",
    "    print(f\"부정정밀도 = {neg_precision}\")\n",
    "    \n",
    "    print(f\"긍정치명오분류율 = {pos_fatal_error}\")\n",
    "    print(f\"부정치명오분류율 = {neg_fatal_error}\")\n",
    "    \n",
    "    print(\"===================================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a16f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "503bf49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[딥서치]\n",
      "               +     0     -  Actual_total\n",
      "+           2200  1333   448          3981\n",
      "0            471  1464   665          2600\n",
      "-            574  2001  4128          6703\n",
      "Pred_total  3245  4798  5241         13284\n",
      "\n",
      "\n",
      "정확도 = 0.5865703101475459\n",
      "f1 score = 0.5105868734517719\n",
      "총민감도 = 0.4498741556169255\n",
      "총정밀도 = 0.5902430617800013\n",
      "긍정민감도 = 0.5526249686008541\n",
      "부정민감도 = 0.6158436520960764\n",
      "긍정정밀도 = 0.6779661016949152\n",
      "부정정밀도 = 0.7876359473382942\n",
      "긍정치명오분류율 = 0.11253453906053755\n",
      "부정치명오분류율 = 0.08563329852304938\n",
      "===================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(1, epochs + 1):\n",
    "    \n",
    "cm = pd.DataFrame(np.array([[2200, 1333, 448],\n",
    "                          [471, 1464, 665],\n",
    "                          [574, 2001, 4128]]))\n",
    "cm.loc['Pred_Total']= cm.sum()\n",
    "cm.loc[:,'Acutal_Total'] = cm.sum(axis=1)\n",
    "    \n",
    "cm.columns = ['+', '0', '-', 'Actual_total']\n",
    "cm.index = ['+', '0', '-', 'Pred_total']\n",
    "    \n",
    "    \n",
    "pos_sensitivity = cm.iloc[0, 0] / cm.iloc[0, 3]\n",
    "neu_sensitivity = cm.iloc[1, 0] / cm.iloc[1, 3]\n",
    "neg_sensitivity = cm.iloc[2, 2] / cm.iloc[2, 3]\n",
    "    \n",
    "    \n",
    "pos_precision = cm.iloc[0, 0] / cm.iloc[3, 0]\n",
    "neu_precision = cm.iloc[1, 1] / cm.iloc[3, 1]\n",
    "neg_precision = cm.iloc[2, 2] / cm.iloc[3, 2]\n",
    "    \n",
    "pos_fatal_error = cm.iloc[0, 2] / cm.iloc[0, 3]\n",
    "neg_fatal_error = cm.iloc[2, 0] / cm.iloc[2, 3]\n",
    "    \n",
    "accuracy = (cm.iloc[0, 0] + cm.iloc[1, 1] + cm.iloc[2, 2]) / cm.iloc[3, 3]\n",
    "sensitivity = (pos_sensitivity + neu_sensitivity + neg_sensitivity) / 3\n",
    "precision = (pos_precision + neu_precision + neg_precision) / 3\n",
    "    \n",
    "f1_score = 2 * ((precision * sensitivity) / (precision + sensitivity))\n",
    "    \n",
    "    \n",
    "print(f\"[딥서치]\")\n",
    "print(cm)\n",
    "print(\"\\n\")\n",
    "print(f\"정확도 = {accuracy}\")\n",
    "print(f\"f1 score = {f1_score}\")\n",
    "print(f\"총민감도 = {sensitivity}\")\n",
    "print(f\"총정밀도 = {precision}\")\n",
    "    \n",
    "print(f\"긍정민감도 = {pos_sensitivity}\")\n",
    "print(f\"부정민감도 = {neg_sensitivity}\")\n",
    "    \n",
    "print(f\"긍정정밀도 = {pos_precision}\")\n",
    "print(f\"부정정밀도 = {neg_precision}\")\n",
    "    \n",
    "print(f\"긍정치명오분류율 = {pos_fatal_error}\")\n",
    "print(f\"부정치명오분류율 = {neg_fatal_error}\")\n",
    "    \n",
    "print(\"===================================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5f04a2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[다음소프트]\n",
      "               +     0     -  Actual_total\n",
      "+           3008   753   235          3996\n",
      "0            799   645   774          2218\n",
      "-            404  1059  5583          7046\n",
      "Pred_total  4211  2457  6592         13260\n",
      "\n",
      "\n",
      "정확도 = 0.6965309200603318\n",
      "f1 score = 0.621222918751525\n",
      "총민감도 = 0.6351172201017534\n",
      "총정밀도 = 0.6079235270558403\n",
      "긍정민감도 = 0.7527527527527528\n",
      "부정민감도 = 0.7923644621061595\n",
      "긍정정밀도 = 0.7143196390406079\n",
      "부정정밀도 = 0.8469356796116505\n",
      "긍정치명오분류율 = 0.05880880880880881\n",
      "부정치명오분류율 = 0.0573374964518876\n",
      "===================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(1, epochs + 1):\n",
    "    \n",
    "cm = pd.DataFrame(np.array([[3008, 753, 235],\n",
    "                          [799, 645, 774],\n",
    "                          [404, 1059, 5583]]))\n",
    "cm.loc['Pred_Total']= cm.sum()\n",
    "cm.loc[:,'Acutal_Total'] = cm.sum(axis=1)\n",
    "    \n",
    "cm.columns = ['+', '0', '-', 'Actual_total']\n",
    "cm.index = ['+', '0', '-', 'Pred_total']\n",
    "    \n",
    "    \n",
    "pos_sensitivity = cm.iloc[0, 0] / cm.iloc[0, 3]\n",
    "neu_sensitivity = cm.iloc[1, 0] / cm.iloc[1, 3]\n",
    "neg_sensitivity = cm.iloc[2, 2] / cm.iloc[2, 3]\n",
    "    \n",
    "    \n",
    "pos_precision = cm.iloc[0, 0] / cm.iloc[3, 0]\n",
    "neu_precision = cm.iloc[1, 1] / cm.iloc[3, 1]\n",
    "neg_precision = cm.iloc[2, 2] / cm.iloc[3, 2]\n",
    "    \n",
    "pos_fatal_error = cm.iloc[0, 2] / cm.iloc[0, 3]\n",
    "neg_fatal_error = cm.iloc[2, 0] / cm.iloc[2, 3]\n",
    "    \n",
    "accuracy = (cm.iloc[0, 0] + cm.iloc[1, 1] + cm.iloc[2, 2]) / cm.iloc[3, 3]\n",
    "sensitivity = (pos_sensitivity + neu_sensitivity + neg_sensitivity) / 3\n",
    "precision = (pos_precision + neu_precision + neg_precision) / 3\n",
    "    \n",
    "f1_score = 2 * ((precision * sensitivity) / (precision + sensitivity))\n",
    "    \n",
    "    \n",
    "print(\"[다음소프트]\")\n",
    "print(cm)\n",
    "print(\"\\n\")\n",
    "print(f\"정확도 = {accuracy}\")\n",
    "print(f\"f1 score = {f1_score}\")\n",
    "print(f\"총민감도 = {sensitivity}\")\n",
    "print(f\"총정밀도 = {precision}\")\n",
    "    \n",
    "print(f\"긍정민감도 = {pos_sensitivity}\")\n",
    "print(f\"부정민감도 = {neg_sensitivity}\")\n",
    "    \n",
    "print(f\"긍정정밀도 = {pos_precision}\")\n",
    "print(f\"부정정밀도 = {neg_precision}\")\n",
    "    \n",
    "print(f\"긍정치명오분류율 = {pos_fatal_error}\")\n",
    "print(f\"부정치명오분류율 = {neg_fatal_error}\")\n",
    "    \n",
    "print(\"===================================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678d5169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019de1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b3a75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
